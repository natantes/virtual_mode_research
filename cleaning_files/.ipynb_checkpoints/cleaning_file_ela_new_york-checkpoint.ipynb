{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "369252bf",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ff7014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import pyodbc\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad57591",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3ab72f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the years for each dataframe\n",
    "years = [2019, 2021, 2022]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3c2a79",
   "metadata": {},
   "source": [
    "# Intialize Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e58529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_convert(val):\n",
    "    try:\n",
    "        return int(val)\n",
    "    except ValueError:\n",
    "        print(f\"Value {val} can't be converted to int\")\n",
    "        return None\n",
    "    \n",
    "def import_mdb(MDBs, DRV, PWD, NAMES):\n",
    "    \n",
    "    databases = {}\n",
    "    \n",
    "    for MDB, NAME in zip(MDBs, NAMES):\n",
    "        # connect to db\n",
    "        con = pyodbc.connect('DRIVER={};DBQ={};PWD={}'.format(DRV,MDB,PWD))\n",
    "        cur = con.cursor()\n",
    "\n",
    "        # List all tables in the database\n",
    "        tables = list(map(lambda t: t.table_name, con.cursor().tables(tableType='TABLE')))\n",
    "\n",
    "        # Initialize an empty dictionary to hold your dataframes and databases\n",
    "        database = {}\n",
    "\n",
    "        # Try to read each table one by one\n",
    "        for table in tables:\n",
    "            try:\n",
    "                df = pd.read_sql(f'SELECT * FROM [{table}]', con)  # enclose table name in brackets\n",
    "                database[table] = df\n",
    "                print(f\"Successfully read table: {table}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to read table: {table}\")\n",
    "                print(f\"Error: {e}\")\n",
    "        databases[NAME] = database\n",
    "        \n",
    "    return databases\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore', 'pandas only support SQLAlchemy connectable.*')\n",
    "warnings.filterwarnings('ignore', category=pd.errors.DtypeWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.core.common.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c24ee0",
   "metadata": {},
   "source": [
    "# Import Main Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c062de15",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/SRC2019/ARE2019.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8y/bjy80p890fg6hgk57b8wz5d40000gn/T/ipykernel_58919/1013834716.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAIN_NAMES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAIN_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Annual Regents Exams'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthousands\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Main data Windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/SRC2019/ARE2019.csv'"
     ]
    }
   ],
   "source": [
    "# Main Data Filepaths WINDOWS\n",
    "# MAIN_PATH = [\n",
    "#                '../data/SRC2019/SRC2019.mdb;',\n",
    "#                '../data/SRC2021/SRC2021.mdb;',\n",
    "#                '../data/SRC2022/SRC2022.mdb;',\n",
    "#               ]\n",
    "\n",
    "# Main Data Filepaths MAC\n",
    "MAIN_PATH = [\n",
    "               '.../data/SRC2019/ARE2019.csv',\n",
    "               '../data/SRC2021/ARE2021.csv',\n",
    "               '../data/SRC2022/ARE2022.CSV',\n",
    "              ]\n",
    "MAIN_NAMES = [\n",
    "               'MAIN2019',\n",
    "               'MAIN2021',\n",
    "               'MAIN2022',\n",
    "              ]\n",
    "\n",
    "main_data = {}\n",
    "for name, filepath in zip(MAIN_NAMES, MAIN_PATH):\n",
    "    main_data[name] = {'Annual Regents Exams':pd.read_csv(filepath, thousands=',')}\n",
    "\n",
    "# Main data Windows\n",
    "# main_data = import_mdb(MAIN_PATH, DRV, PWD, MAIN_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b55ae7",
   "metadata": {},
   "source": [
    "# Import Enrollment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54b123b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DRV = '{Microsoft Access Driver (*.mdb, *.accdb)}'\n",
    "# PWD = 'pw'\n",
    "\n",
    "# # Enroll Data Filepaths\n",
    "# ENROLL_PATH = [\n",
    "#                '../data/enrollment_2019/ENROLL2019.mdb;',\n",
    "#                '../data/enrollment_2021/ENROLL2021.mdb;',\n",
    "#                '../data/enrollment_2022/ENROLL2022.mdb;',\n",
    "#               ]\n",
    "# ENROLL_NAMES = [\n",
    "#                'ENROLL2019',\n",
    "#                'ENROLL2021',\n",
    "#                'ENROLL2022',\n",
    "#               ]\n",
    "\n",
    "\n",
    "# enroll_data = import_mdb(ENROLL_PATH, DRV, PWD, ENROLL_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac07e7d",
   "metadata": {},
   "source": [
    "# Import Dropout Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886dc70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data22 = pd.read_csv(\"./raw_data/GRAD_RATE_AND_OUTCOMES_2022.csv\", thousands=',')\n",
    "data21 = pd.read_csv(\"./raw_data/GRAD_RATE_AND_OUTCOMES_2021.csv\", thousands=',')\n",
    "data19 = pd.read_csv(\"./raw_data/GRAD_RATE_AND_OUTCOMES_2019.csv\", thousands=',')\n",
    "\n",
    "dropout_dfs = [data19, data21, data22]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbf9734",
   "metadata": {},
   "source": [
    "# Combine Dropout Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0a147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove districts, only keep schools\n",
    "for i, df in enumerate(dropout_dfs):\n",
    "    dropout_dfs[i] = df[df['aggregation_type'] == 'School']\n",
    "\n",
    "#   Only keep schools which are present in all years    #\n",
    "#########################################################\n",
    "\n",
    "# Convert the 'ID' column of each DataFrame to a set\n",
    "set1 = set(dropout_dfs[0]['aggregation_code'])\n",
    "set2 = set(dropout_dfs[1]['aggregation_code'])\n",
    "set3 = set(dropout_dfs[2]['aggregation_code'])\n",
    "\n",
    "# Find the intersection of all 4 sets - i.e., the common IDs\n",
    "common_ids = set1 & set2 & set3\n",
    "\n",
    "# Filter each DataFrame to only include rows with a common ID\n",
    "for i, df in enumerate(dropout_dfs):\n",
    "    dropout_dfs[i] = df[df['aggregation_code'].isin(common_ids)]\n",
    "    \n",
    "###########################################################\n",
    "\n",
    "common_ids = set(df['aggregation_code'])\n",
    "\n",
    "# Initialize a list to store the updated dataframes\n",
    "updated_dfs = []\n",
    "\n",
    "# Iterate over the dropout dataframes and the years together\n",
    "for year, df in zip(years, dropout_dfs):\n",
    "    # Add a new column 'year' to the dataframe\n",
    "    df['year'] = year\n",
    "    # Append the updated dataframe to the list\n",
    "    updated_dfs.append(df)\n",
    "\n",
    "# Concatenate the updated dataframes together\n",
    "dropout_df = pd.concat(updated_dfs)\n",
    "\n",
    "# drop disttricts from the dataframe\n",
    "dropout_df = dropout_df[~dropout_df['aggregation_code'].astype(str).str.endswith('0000.0')]\n",
    "\n",
    "# Reset the index of the combined dataframe\n",
    "dropout_df = dropout_df.reset_index(drop=True)\n",
    "\n",
    "common_ids = set(dropout_df['aggregation_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ee3a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check if 'comparison' year is in 'membership_desc'\n",
    "def check_year_in_desc(row):\n",
    "    return str(row['comparison']) in row['membership_desc'] and \"August\" not in row['membership_desc']\n",
    "\n",
    "dropout_df = dropout_df[dropout_df['subgroup_name'] == 'All Students']\n",
    "dropout_df['report_school_year'] = dropout_df['report_school_year'].apply(lambda x: int(str(x).split('-')[1]))\n",
    "dropout_df['report_school_year'] = dropout_df['report_school_year'].apply(lambda x: x + 2000 if x < 100 else x)\n",
    "\n",
    "# Convert the 'report_school_year' to int and subtract 4\n",
    "dropout_df['comparison'] = dropout_df['report_school_year'] - 4\n",
    "\n",
    "# Apply the function to each row of dropout_df\n",
    "dropout_df = dropout_df[dropout_df.apply(check_year_in_desc, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3035d0d2",
   "metadata": {},
   "source": [
    "# Combine Main Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81073e6",
   "metadata": {},
   "source": [
    "### Remove Districts, Keep Schools Common Across Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca01e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_entity_ids = set(main_data['MAIN2019']['Annual Regents Exams']['ENTITY_CD']) \n",
    "\n",
    "for database in main_data:\n",
    "    current_data = main_data[database]['Annual Regents Exams']\n",
    "    current_data['ENTITY_CD'] = current_data['ENTITY_CD'].apply(safe_convert)\n",
    "    current_data = current_data[~current_data['ENTITY_CD'].astype(str).str.endswith('0000')]\n",
    "\n",
    "    common_entity_ids = set(current_data['ENTITY_CD']) & common_ids\n",
    "    \n",
    "for database in main_data:\n",
    "    current_data = main_data[database]['Annual Regents Exams']\n",
    "    main_data[database]['Annual Regents Exams'] = current_data[current_data['ENTITY_CD'].isin(common_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acd889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for database in main_data:\n",
    "    current_data = main_data[database]['Annual Regents Exams']\n",
    "    main_data[database] = {subject: current_data[current_data['SUBJECT'] == subject] for subject in current_data['SUBJECT'].unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a7edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_to_new = {\n",
    "    'REG_PHYS_PS':'Regents Phy Set/Physics',\n",
    "    'REG_NF_GLHIST':'Regents NF Global History',\n",
    "    'REG_COMENG':'Regents Common Core English Language Art', \n",
    "    'REG_ESCI_PS':'Regents Phy Set/Earth Sci',\n",
    "    'REG_CHEM_PS':'Regents Phy Set/Chemistry', \n",
    "    'REG_COMALG1':'Regents Common Core Algebra I', \n",
    "    'REG_COMGEOM':'Regents Common Core Geometry', \n",
    "    'REG_LENV':'Regents Living Environment',\n",
    "    'REG_USHG_RV':\"Regents US History&Gov't\"\n",
    "}\n",
    "\n",
    "new_to_old = {}\n",
    "\n",
    "for key in old_to_new:\n",
    "    new_to_old[old_to_new[key]] = key\n",
    "\n",
    "tests = (set(old_to_new[test] for test in old_to_new) \n",
    "         & set(test for test in main_data['MAIN2021']) \n",
    "         & set(test for test in main_data['MAIN2022']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff0e2e7",
   "metadata": {},
   "source": [
    "### Calculate Demographic Percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5856ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, database in zip(years, MAIN_NAMES):\n",
    "    current_df = None\n",
    "    current_df = (main_data[database]['Regents Common Core English Language Art'] \n",
    "                  if 'Regents Common Core English Language Art' in main_data[database]\n",
    "                else main_data[database][new_to_old['Regents Common Core English Language Art']])\n",
    "\n",
    "    # Filter the DataFrame to only include rows where SUBGROUP_NAME == 'All Students'\n",
    "    total_students_df = current_df[(current_df['SUBGROUP_NAME'] == 'All Students') & (current_df['YEAR'] == year)][['ENTITY_CD', 'TESTED', 'YEAR']]\n",
    "    print(\"checkpoint 1\")\n",
    "\n",
    "    # Merge the total students for 'All Students' back into the original DataFrame\n",
    "    current_df = pd.merge(current_df, total_students_df, on=['ENTITY_CD', 'YEAR'], how='left', suffixes=('', '_total'))\n",
    "    print(\"checkpoint 2\")\n",
    "\n",
    "    # List of subgroups of interest\n",
    "    KEPT_SUBGROUPS = ['Male', 'Female', 'White', 'Hispanic or Latino', 'Black or African American', 'Asian or Native Hawaiian/Other Pacific Islander','Economically Disadvantaged']\n",
    "\n",
    "    # List to store DataFrames\n",
    "    df_list = []\n",
    "    columns = \"ENTITY_CD  ENTITY_NAME YEAR SUBJECT TESTED TESTED_total NUM_LEVEL1 PER_LEVEL1 NUM_LEVEL2 PER_LEVEL2 NUM_LEVEL3 PER_LEVEL3 NUM_LEVEL4 PER_LEVEL4 NUM_LEVEL5 PER_LEVEL5 NUM_PROF PER_PROF\"\n",
    "    columns = columns.split()\n",
    "\n",
    "    # Get ENTITY_NAME for 'All Students' subgroup\n",
    "    multiple_df = current_df[current_df['SUBGROUP_NAME'] == 'All Students'][columns]\n",
    "    print(\"checkpoint 3\")\n",
    "\n",
    "    # Loop over each subgroup and calculate percentage\n",
    "    for subgroup in KEPT_SUBGROUPS:\n",
    "        temp_df = current_df[(current_df['SUBGROUP_NAME'] == subgroup) & (current_df['YEAR'] == year)].copy()\n",
    "        subgroup = subgroup.upper()\n",
    "        temp_df[subgroup + '_PCT'] = temp_df['TESTED'] / temp_df['TESTED_total'] * 100\n",
    "        temp_df = temp_df[['ENTITY_CD', subgroup + '_PCT']]  # Keep 'ENTITY_CD' in each temp_df\n",
    "        df_list.append(temp_df)\n",
    "    print(\"checkpoint 4\")\n",
    "\n",
    "    # Merge all DataFrames on ENTITY_CD\n",
    "    result_df = multiple_df\n",
    "    for temp_df in df_list:\n",
    "        result_df = result_df.merge(temp_df, on='ENTITY_CD', how='outer')\n",
    "    print(\"checkpoint 5\")\n",
    "\n",
    "\n",
    "    # Drop observations where TESTED is less than 4\n",
    "    result_df = result_df[result_df['TESTED_total'] >= 2]\n",
    "    print(\"checkpoint 6\")\n",
    "\n",
    "    # Fill NaN values with 0\n",
    "    result_df = result_df.fillna(0)\n",
    "    print(\"checkpoint 7\")\n",
    "\n",
    "    cols = \"NUM_LEVEL1 PER_LEVEL1 NUM_LEVEL2 PER_LEVEL2 NUM_LEVEL3 PER_LEVEL3 NUM_LEVEL4 PER_LEVEL4 NUM_LEVEL5 PER_LEVEL5 NUM_PROF PER_PROF\".split()\n",
    "    for col in cols:\n",
    "        result_df[col] = result_df[col].replace('s', 0)\n",
    "    print(\"checkpoint 7\")\n",
    "    if 'Regents Common Core English Language Art' in main_data[database]:\n",
    "        main_data[database]['Regents Common Core English Language Art'] = result_df  # store results in new dictionary instead of main_data\n",
    "    else:\n",
    "        main_data[database][new_to_old['Regents Common Core English Language Art']] = result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0335f98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_df = None\n",
    "# if 'Regents Common Core English Language Art' in main_data['MAIN2021']:\n",
    "#     current_df = main_data['MAIN2021']['Regents Common Core English Language Art']\n",
    "# else:\n",
    "#     current_df = main_data['MAIN2021'][new_to_old['Regents Common Core English Language Art']]\n",
    "\n",
    "# # Filter the DataFrame to only include rows where SUBGROUP_NAME == 'All Students'\n",
    "# total_students_df = current_df[(current_df['SUBGROUP_NAME'] == 'All Students') & (current_df['YEAR'] == 2021)][['ENTITY_CD', 'TESTED', 'YEAR']]\n",
    "\n",
    "# # Merge the total students for 'All Students' back into the original DataFrame\n",
    "# current_df = pd.merge(current_df, total_students_df, on=['ENTITY_CD', 'YEAR'], how='left', suffixes=('', '_total'))\n",
    "\n",
    "# # List of subgroups of interest\n",
    "# KEPT_SUBGROUPS = ['Male', 'Female', 'White', 'Hispanic or Latino', 'Black or African American', 'Asian or Native Hawaiian/Other Pacific Islander','Economically Disadvantaged']\n",
    "\n",
    "# # List to store DataFrames\n",
    "# df_list = []\n",
    "# columns = \"ENTITY_CD  ENTITY_NAME YEAR SUBJECT TESTED TESTED_total NUM_LEVEL1 PER_LEVEL1 NUM_LEVEL2 PER_LEVEL2 NUM_LEVEL3 PER_LEVEL3 NUM_LEVEL4 PER_LEVEL4 NUM_LEVEL5 PER_LEVEL5 NUM_PROF PER_PROF\"\n",
    "# columns = columns.split()\n",
    "\n",
    "# # Get ENTITY_NAME for 'All Students' subgroup\n",
    "# multiple_df = current_df[current_df['SUBGROUP_NAME'] == 'All Students'][columns]\n",
    "\n",
    "# # Loop over each subgroup and calculate percentage\n",
    "# for subgroup in KEPT_SUBGROUPS:\n",
    "#     temp_df = current_df[(current_df['SUBGROUP_NAME'] == subgroup) & (current_df['YEAR'] == 2021)].copy()\n",
    "#     subgroup = subgroup.upper()\n",
    "#     temp_df[subgroup + '_PCT'] = temp_df['TESTED'] / temp_df['TESTED_total'] * 100\n",
    "#     temp_df = temp_df[['ENTITY_CD', subgroup + '_PCT']]  # Keep 'ENTITY_CD' in each temp_df\n",
    "#     df_list.append(temp_df)\n",
    "\n",
    "# # Merge all DataFrames on ENTITY_CD\n",
    "# result_df = multiple_df\n",
    "# for temp_df in df_list:\n",
    "#     result_df = result_df.merge(temp_df, on='ENTITY_CD', how='outer')\n",
    "    \n",
    "\n",
    "# # Drop observations where TESTED is less than 4\n",
    "# result_df = result_df[result_df['TESTED_total'] >= 2]\n",
    "\n",
    "# # Fill NaN values with 0\n",
    "# result_df = result_df.fillna(0)\n",
    "\n",
    "# cols = \"NUM_LEVEL1 PER_LEVEL1 NUM_LEVEL2 PER_LEVEL2 NUM_LEVEL3 PER_LEVEL3 NUM_LEVEL4 PER_LEVEL4 NUM_LEVEL5 PER_LEVEL5 NUM_PROF PER_PROF\".split()\n",
    "# for col in cols:\n",
    "#     result_df[col] = result_df[col].replace('s', 0)\n",
    "\n",
    "# main_data['MAIN2021']['Regents Common Core English Language Art'] = result_df  # store results in new dictionary instead of main_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5968b520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# current_df = None\n",
    "# if 'Regents Common Core English Language Art' in main_data['MAIN2022']:\n",
    "#     current_df = main_data['MAIN2022']['Regents Common Core English Language Art']\n",
    "# else:\n",
    "#     current_df = main_data['MAIN2022'][new_to_old['Regents Common Core English Language Art']]\n",
    "\n",
    "# # Filter the DataFrame to only include rows where SUBGROUP_NAME == 'All Students'\n",
    "# total_students_df = current_df[(current_df['SUBGROUP_NAME'] == 'All Students') & (current_df['YEAR'] == 2022)][['ENTITY_CD', 'TESTED', 'YEAR']]\n",
    "\n",
    "# # Merge the total students for 'All Students' back into the original DataFrame\n",
    "# current_df = pd.merge(current_df, total_students_df, on=['ENTITY_CD', 'YEAR'], how='left', suffixes=('', '_total'))\n",
    "\n",
    "# # List of subgroups of interest\n",
    "# KEPT_SUBGROUPS = ['Male', 'Female', 'White', 'Hispanic or Latino', 'Black or African American', 'Asian or Native Hawaiian/Other Pacific Islander','Economically Disadvantaged']\n",
    "\n",
    "# # List to store DataFrames\n",
    "# df_list = []\n",
    "# columns = \"ENTITY_CD  ENTITY_NAME YEAR SUBJECT TESTED TESTED_total NUM_LEVEL1 PER_LEVEL1 NUM_LEVEL2 PER_LEVEL2 NUM_LEVEL3 PER_LEVEL3 NUM_LEVEL4 PER_LEVEL4 NUM_LEVEL5 PER_LEVEL5 NUM_PROF PER_PROF\"\n",
    "# columns = columns.split()\n",
    "\n",
    "# # Get ENTITY_NAME for 'All Students' subgroup\n",
    "# multiple_df = current_df[current_df['SUBGROUP_NAME'] == 'All Students'][columns]\n",
    "\n",
    "# # Loop over each subgroup and calculate percentage\n",
    "# for subgroup in KEPT_SUBGROUPS:\n",
    "#     temp_df = current_df[(current_df['SUBGROUP_NAME'] == subgroup) & (current_df['YEAR'] == 2022)].copy()\n",
    "#     subgroup = subgroup.upper()\n",
    "#     temp_df[subgroup + '_PCT'] = temp_df['TESTED'] / temp_df['TESTED_total'] * 100\n",
    "#     temp_df = temp_df[['ENTITY_CD', subgroup + '_PCT']]  # Keep 'ENTITY_CD' in each temp_df\n",
    "#     df_list.append(temp_df)\n",
    "\n",
    "# # Merge all DataFrames on ENTITY_CD\n",
    "# result_df = multiple_df\n",
    "# for temp_df in df_list:\n",
    "#     result_df = result_df.merge(temp_df, on='ENTITY_CD', how='outer')\n",
    "    \n",
    "\n",
    "# # Drop observations where TESTED is less than 4\n",
    "# result_df = result_df[result_df['TESTED_total'] >= 2]\n",
    "\n",
    "# # Fill NaN values with 0\n",
    "# result_df = result_df.fillna(0)\n",
    "\n",
    "# cols = \"NUM_LEVEL1 PER_LEVEL1 NUM_LEVEL2 PER_LEVEL2 NUM_LEVEL3 PER_LEVEL3 NUM_LEVEL4 PER_LEVEL4 NUM_LEVEL5 PER_LEVEL5 NUM_PROF PER_PROF\".split()\n",
    "# for col in cols:\n",
    "#     result_df[col] = result_df[col].replace('s', 0)\n",
    "\n",
    "# main_data['MAIN2022']['Regents Common Core English Language Art'] = result_df  # store results in new dictionary instead of main_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32442ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_df = None\n",
    "# if 'Regents Common Core English Language Art' in main_data['MAIN2019']:\n",
    "#     current_df = main_data['MAIN2019']['Regents Common Core English Language Art']\n",
    "# else:\n",
    "#     current_df = main_data['MAIN2019'][new_to_old['Regents Common Core English Language Art']]\n",
    "\n",
    "# # Filter the DataFrame to only include rows where SUBGROUP_NAME == 'All Students'\n",
    "# total_students_df = current_df[(current_df['SUBGROUP_NAME'] == 'All Students') & (current_df['YEAR'] == 2019)][['ENTITY_CD', 'TESTED', 'YEAR']]\n",
    "\n",
    "# # Merge the total students for 'All Students' back into the original DataFrame\n",
    "# current_df = pd.merge(current_df, total_students_df, on=['ENTITY_CD', 'YEAR'], how='left', suffixes=('', '_total'))\n",
    "\n",
    "# # List of subgroups of interest\n",
    "# KEPT_SUBGROUPS = ['Male', 'Female', 'White', 'Hispanic or Latino', 'Black or African American', 'Asian or Native Hawaiian/Other Pacific Islander','Economically Disadvantaged']\n",
    "\n",
    "# # List to store DataFrames\n",
    "# df_list = []\n",
    "# columns = \"ENTITY_CD  ENTITY_NAME YEAR SUBJECT TESTED TESTED_total NUM_LEVEL1 PER_LEVEL1 NUM_LEVEL2 PER_LEVEL2 NUM_LEVEL3 PER_LEVEL3 NUM_LEVEL4 PER_LEVEL4 NUM_LEVEL5 PER_LEVEL5 NUM_PROF PER_PROF\"\n",
    "# columns = columns.split()\n",
    "\n",
    "# # Get ENTITY_NAME for 'All Students' subgroup\n",
    "# multiple_df = current_df[current_df['SUBGROUP_NAME'] == 'All Students'][columns]\n",
    "\n",
    "# # Loop over each subgroup and calculate percentage\n",
    "# for subgroup in KEPT_SUBGROUPS:\n",
    "#     temp_df = current_df[(current_df['SUBGROUP_NAME'] == subgroup) & (current_df['YEAR'] == 2019)].copy()\n",
    "#     subgroup = subgroup.upper()\n",
    "#     temp_df[subgroup + '_PCT'] = temp_df['TESTED'] / temp_df['TESTED_total'] * 100\n",
    "#     temp_df = temp_df[['ENTITY_CD', subgroup + '_PCT']]  # Keep 'ENTITY_CD' in each temp_df\n",
    "#     df_list.append(temp_df)\n",
    "\n",
    "# # Merge all DataFrames on ENTITY_CD\n",
    "# result_df = multiple_df\n",
    "# for temp_df in df_list:\n",
    "#     result_df = result_df.merge(temp_df, on='ENTITY_CD', how='outer')\n",
    "    \n",
    "\n",
    "# # Drop observations where TESTED is less than 4\n",
    "# result_df = result_df[result_df['TESTED_total'] >= 2]\n",
    "\n",
    "# # Fill NaN values with 0\n",
    "# result_df = result_df.fillna(0)\n",
    "\n",
    "# cols = \"NUM_LEVEL1 PER_LEVEL1 NUM_LEVEL2 PER_LEVEL2 NUM_LEVEL3 PER_LEVEL3 NUM_LEVEL4 PER_LEVEL4 NUM_LEVEL5 PER_LEVEL5 NUM_PROF PER_PROF\".split()\n",
    "# for col in cols:\n",
    "#     result_df[col] = result_df[col].replace('s', 0)\n",
    "\n",
    "# main_data['MAIN2019'][new_to_old['Regents Common Core English Language Art']] = result_df  # store results in new dictionary instead of main_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37693e48",
   "metadata": {},
   "source": [
    "### Concatenate All Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc95651",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.concat([main_data['MAIN2019'][new_to_old['Regents Common Core English Language Art']], main_data['MAIN2021']['Regents Common Core English Language Art'], main_data['MAIN2022']['Regents Common Core English Language Art']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95b6706",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_data['SUBJECT'] = 'Regents Common Core English Language Art'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec2a22d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert 'YEAR' in final_data to int\n",
    "final_data['YEAR'] = final_data['YEAR'].astype(int)\n",
    "\n",
    "# Select specific columns from dropout_df\n",
    "dropout_subset = dropout_df[['aggregation_code', 'report_school_year', 'dropout_pct']]\n",
    "\n",
    "# Merge dropout_df with final_data\n",
    "final_data = pd.merge(final_data, dropout_subset, left_on=['ENTITY_CD', 'YEAR'], right_on=['aggregation_code', 'report_school_year'], how='left')\n",
    "\n",
    "# Replace '-' with np.nan\n",
    "final_data['dropout_pct'] = final_data['dropout_pct'].replace('-', np.nan)\n",
    "\n",
    "# Remove '%' from 'dropout_pct' and convert to float\n",
    "final_data['dropout_pct'] = final_data['dropout_pct'].str.rstrip('%').astype('float')\n",
    "\n",
    "final_data['dropout_pct'] = final_data['dropout_pct'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3135450",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66071e03",
   "metadata": {},
   "source": [
    "# Import Virtual Mode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711b024f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "virtual = pd.read_csv(\"../data/New_York_Schools_LearningModelData_Final.csv\", thousands=',')\n",
    "virtual['Charter'] = virtual['Charter'].replace({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0192c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual = virtual[virtual['TimePeriodStart'].str.endswith(('21', '22'))]\n",
    "virtual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e880c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date columns to datetime format.\n",
    "virtual['TimePeriodStart'] = pd.to_datetime(virtual['TimePeriodStart'])\n",
    "virtual['TimePeriodEnd'] = pd.to_datetime(virtual['TimePeriodEnd'])\n",
    "\n",
    "# Create a new column for year\n",
    "virtual['YEAR'] = virtual['TimePeriodStart'].dt.year\n",
    "\n",
    "# Fill in any missing values in LearningModel with 'InPerson'\n",
    "virtual['LearningModel'] = virtual['LearningModel'].fillna('InPerson')\n",
    "\n",
    "# Replace 'In-person' with 'InPerson'\n",
    "virtual['LearningModel'] = virtual['LearningModel'].replace('In-person', 'InPerson')\n",
    "\n",
    "# Calculate the number of days for each row\n",
    "virtual['Days'] = (virtual['TimePeriodEnd'] - virtual['TimePeriodStart']).dt.days\n",
    "\n",
    "# Group by School, Year, LearningModel, and Charter and sum the number of days\n",
    "grouped = virtual.groupby(['StateAssignedSchoolID', 'YEAR', 'LearningModel', 'Charter', 'DistrictName'])['Days'].sum().reset_index()\n",
    "\n",
    "# Pivot the data so we have separate columns for each learning model\n",
    "pivot = grouped.pivot_table(index=['StateAssignedSchoolID', 'YEAR', 'Charter', 'DistrictName'], columns='LearningModel', values='Days', fill_value=0)\n",
    "\n",
    "# # Group by School, Year, and LearningModel and sum the number of days\n",
    "# grouped = virtual.groupby(['StateAssignedSchoolID', 'YEAR', 'LearningModel'])['Days'].sum().reset_index()\n",
    "\n",
    "# # Pivot the data so we have separate columns for each learning model\n",
    "# pivot = grouped.pivot_table(index=['StateAssignedSchoolID', 'YEAR'], columns='LearningModel', values='Days', fill_value=0)\n",
    "\n",
    "# Reset the index\n",
    "pivot.reset_index(inplace=True)\n",
    "\n",
    "# Calculate the total days in each year\n",
    "pivot['TotalDays'] = pivot['Virtual'] + pivot['Hybrid'] + pivot['InPerson']\n",
    "\n",
    "# Calculate the percentage of days that are virtual and hybrid for each year\n",
    "pivot['VirtualPercent'] = pivot['Virtual'] / pivot['TotalDays']\n",
    "pivot['HybridPercent'] = pivot['Hybrid'] / pivot['TotalDays']\n",
    "\n",
    "# Calculate the score for each year\n",
    "pivot['Score'] = (pivot['Virtual'] + 0.5 * pivot['Hybrid']) / pivot['TotalDays']\n",
    "\n",
    "# Reset the column names after pivot\n",
    "pivot.columns.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf364cdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pivot = pivot.drop(columns=['InPerson', 'Hybrid', 'Virtual', 'TotalDays'])\n",
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096d0fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dropout_df with final_data\n",
    "final_data = pd.merge(final_data, pivot, left_on=['ENTITY_CD', 'YEAR'], right_on=['StateAssignedSchoolID', 'YEAR'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf6270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_data.drop(columns=[col for col in \"ENTITY_NAME TESTED_total NUM_LEVEL1 PER_LEVEL1 NUM_LEVEL2 PER_LEVEL2 NUM_LEVEL3 PER_LEVEL3 NUM_LEVEL4 PER_LEVEL4 NUM_LEVEL5 PER_LEVEL5 NUM_PROF aggregation_code report_school_year StateAssignedSchoolID MALE_PCT FEMALE_PCT dropout_pct SUBJECT\".split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4191fd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename multiple columns\n",
    "final_data = final_data.rename(columns={'ENTITY_CD': 'schoolcode', \n",
    "                        'TESTED': 'totalenroll', \n",
    "                        'PER_PROF': 'elapass', \n",
    "                        'WHITE_PCT': 'white',\n",
    "                        'HISPANIC OR LATINO_PCT': 'hispanic',     \n",
    "                        'BLACK OR AFRICAN AMERICAN_PCT': 'black',         \n",
    "                        'ASIAN OR NATIVE HAWAIIAN/OTHER PACIFIC ISLANDER_PCT': 'asian',                     \n",
    "                        'ECONOMICALLY DISADVANTAGED_PCT': 'lowincome',                                         \n",
    "                        'VirtualPercent': 'virtualper',    \n",
    "                        'HybridPercent': 'hybridper',    \n",
    "                        'Score': 'schoolmode',    \n",
    "                        'YEAR': 'year',    \n",
    "                        'Charter': \"charter\"\n",
    "                        'DistrictName': \"district\",\n",
    "                                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd01aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify 'schoolcode' values of rows in 2021 where 'schoolmode' is NaN\n",
    "schoolcodes_to_remove = final_data.loc[(final_data['year'] == 2021) & (final_data['schoolmode'].isna()), 'schoolcode'].unique()\n",
    "\n",
    "# Remove all rows with those 'schoolcode' values\n",
    "final_data = final_data.loc[~final_data['schoolcode'].isin(schoolcodes_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5827fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_data.fillna(0)\n",
    "final_cols = [\n",
    "'schoolcode',\n",
    " 'year',\n",
    " 'charter',\n",
    " 'elapass',\n",
    " 'schoolmode',\n",
    " 'virtualper',\n",
    " 'hybridper',\n",
    " 'totalenroll',\n",
    " 'lowincome',\n",
    " 'white',\n",
    " 'black',\n",
    " 'hispanic',\n",
    " 'asian',\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_data[final_cols]\n",
    "rounding_cols = [\n",
    " 'schoolmode',\n",
    " 'virtualper',\n",
    " 'hybridper',\n",
    " 'totalenroll',\n",
    " 'lowincome',\n",
    " 'white',\n",
    " 'black',\n",
    " 'hispanic',\n",
    " 'asian',\n",
    "      ]\n",
    "\n",
    "final_data[rounding_cols] = final_data[rounding_cols].round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97749b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data by school and year\n",
    "final_data.sort_values(['schoolcode', 'year'], inplace=True)\n",
    "\n",
    "# Replace 0 with NaN in 'district' column\n",
    "final_data['district'].replace(0, pd.NA, inplace=True)\n",
    "\n",
    "# Now apply the 'ffill' and 'bfill' methods\n",
    "final_data['district'] = final_data.groupby('schoolcode')['district'].apply(lambda group: group.ffill().bfill())\n",
    "\n",
    "final_data = final_data.dropna(subset=['district'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e609f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9df1820",
   "metadata": {},
   "source": [
    "# Export NYC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_export = final_data[final_data[\"year\"] != 2022]\n",
    "\n",
    "first_export.to_csv(\"./final_data_components/elapass_new_york.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c42429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data.to_csv(\"for_running.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
